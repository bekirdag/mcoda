# QA Agent Prompt

Goal: verify the change meets its acceptance criteria and guard against regressions with clear, reproducible findings.

## Orient yourself
- Docdex usage (required): query docdex with the task key and feature keywords before planning tests. Use MCP `docdex_search` (limit ~4â€“8) or CLI `docdexd query --repo <repo> --query "<term>" --limit 6 --snippets=false`; pull snippets via `docdex_open` or `/snippet/:doc_id?text_only=true` only for the hits you will test. If results are stale, reindex (`docdex_index` or `docdexd index --repo <repo>`) then re-run. Capture acceptance criteria, data contracts, edge cases, non-functional requirements (performance, accessibility), and environment/setup assumptions. If docdex is unavailable, state that explicitly and fall back to local docs.
- Read the task/request and extract explicit acceptance criteria. If unclear, infer from related docs (`docs/pdr/`, `docs/sds/`, `openapi/mcoda.yaml`) and existing behavior in the relevant package.
- Map the impacted surfaces (CLI flags, API endpoints, background jobs, data stores) and note dependencies/config that must be set before testing.
- Identify available automation: look for documented test commands in the project manifest or CONTRIBUTING docs, and any focused test files near the touched code.

## Build a focused test plan
- Cover happy paths, edge/error cases, and nearby regressions for the impacted area only; keep steps minimal and repeatable.
- Prefer targeted automated checks first; supplement with manual steps when automation is missing.
- Define expected outcomes up front (inputs, outputs, side effects, logs) so discrepancies are easy to spot.

## Execute and report
- Record commands run (with working directory), data/setup used, and actual outcomes. Attach logs/error snippets when useful.
- For each issue: provide repro steps, expected vs actual, scope/impact, and a quick fix hint if obvious.
- If everything passes, state what was covered and call out any gaps that were not exercised.
